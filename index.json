[{"categories":null,"content":"It's a example","date":"2020-09-14","objectID":"/2020-09-14/","tags":["python"],"title":"Pytorch視覺化","uri":"/2020-09-14/"},{"categories":null,"content":" Pytorch相對於tensorflow不一樣的地方是不是用tf.variable來分類圖，而是包在Tensor裏面，這就會讓tensorboard相對起來難讀一點，不過要讓可讀性增加還是要從程式下手，另外Autograd也是它特別的地方。 ","date":"2020-09-14","objectID":"/2020-09-14/:0:0","tags":["python"],"title":"Pytorch視覺化","uri":"/2020-09-14/"},{"categories":null,"content":"Tensorboard 生成圖 from torch.utils.tensorboard import SummaryWriter # from tensorboardX import SummaryWriter # or use this # Writer will output to ./runs/ directory by default writer = SummaryWriter() # Initial the Network model (class) # writer.add_image('images', grid, 0) # writer.add_figure(\"matplotlib/figure\", figure) writer.add_graph(model, images) writer.close() 執行結束後，預設儲存在./runs/ tensorboard --logdir=\"./runs/\" ","date":"2020-09-14","objectID":"/2020-09-14/:1:0","tags":["python"],"title":"Pytorch視覺化","uri":"/2020-09-14/"},{"categories":null,"content":"torchvision生成graphviz流程圖 outputs = model(inputs) from torchviz import make_dot, make_dot_from_trace for param in model.named_parameters(): print(param[0]) vis_graph = make_dot(outputs, params=dict(model.named_parameters())) vis_graph.view() # \"Digraph.gv.pdf\" ","date":"2020-09-14","objectID":"/2020-09-14/:2:0","tags":["python"],"title":"Pytorch視覺化","uri":"/2020-09-14/"},{"categories":null,"content":"Save \u0026 Load weight 在這個儲存weight、bias的部分，tensorflow v1是相對起來比較簡單的，直接存成*.meta還有另外兩個檔案，直接使用FileLoader讀取。 Pytorch使用類似Pickle的方式儲存Weight，他們讓model下面多了一個屬性儲存現在的weight，然後還有兩個方法可以把weight重新加載到模型上面。 跟tensorflow v1比起來，就是原本的Model的網路網路（Class）要留着，不能像是tensorflow v1直接使用meta讀取Model Graph，缺點就是不能藏程式吧！ # Save weight # Print model's state_dict print(\"Model's state_dict:\") for param_tensor in model.state_dict(): print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size()) # Print optimizer's state_dict print(\"Optimizer's state_dict:\") for var_name in optimizer.state_dict(): print(var_name, \"\\t\", optimizer.state_dict()[var_name]) torch.save(model, 'net.pt') torch.save(model.state_dict(), 'net_params.pt') torch.save(optimizer, 'optimizer.pt') torch.save(optimizer.state_dict(), 'optimizer_params.pt') 加載 model.load_state_dict(torch.load('net_params.pt')) model.eval() 用法差不多這樣，print出來的結果差不多是這樣，去測試咯 conv1.weight torch.Size([6, 3, 5, 5]) conv1.bias torch.Size([6]) conv2.weight torch.Size([16, 6, 5, 5]) conv2.bias torch.Size([16]) fc1.weight torch.Size([120, 400]) fc1.bias torch.Size([120]) fc2.weight torch.Size([84, 120]) fc2.bias torch.Size([84]) fc3.weight torch.Size([10, 84]) fc3.bias torch.Size([10]) Optimizer's state_dict: state {} param_groups [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4675713712, 4675713784, 4675714000, 4675714072, 4675714216, 4675714288, 4675714432, 4675714504, 4675714648, 4675714720]}] 如果還要儲存現在的計算過程，分很多次計算的話可以使用checkpoint的方式儲存 torch.save({ 'epoch': epoch+1, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, ... }, PATH) 加載 model = TheModelClass(*args, **kwargs) optimizer = TheOptimizerClass(*args, **kwargs) checkpoint = torch.load(PATH) model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) epoch = checkpoint['epoch'] loss = checkpoint['loss'] model.eval() # - or - model.train() ","date":"2020-09-14","objectID":"/2020-09-14/:3:0","tags":["python"],"title":"Pytorch視覺化","uri":"/2020-09-14/"},{"categories":null,"content":"Torchvision 網路 AlexNet VGG ResNet SqueezeNet DenseNet Inception v3 GoogLeNet ShuffleNet v2 MobileNet v2 ResNeXt Wide ResNet MNASNet import torch import torch.nn as nn # Layer import torch.nn.functional as F # Function import torchvision model = torchvision.models.alexnet() ","date":"2020-09-14","objectID":"/2020-09-14/:4:0","tags":["python"],"title":"Pytorch視覺化","uri":"/2020-09-14/"},{"categories":null,"content":"Custom Layer 參考GitHub Sébastien M. P.的程式 在建構子構建我們用到的Layer class，然後在forward定義計算流程 其中ComplexFunction.complex_relu def complex_relu(input_r,input_i): return relu(input_r), relu(input_i) 其中 complexLayers.ComplexBatchNorm2d class ComplexConv2d(Module): def __init__(self,in_channels, out_channels, kernel_size=3, stride=1, padding = 0, dilation=1, groups=1, bias=True): super(ComplexConv2d, self).__init__() self.conv_r = Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias) self.conv_i = Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias) def forward(self,input_r, input_i): # assert(input_r.size() == input_i.size()) return self.conv_r(input_r)-self.conv_i(input_i), \\ self.conv_r(input_i)+self.conv_i(input_r) Class跟Function的差別主要是，Class要做實體化的動作，所以要在建構子的地方做實體化，比如ComplexConv2d如果我要使用的話就要在Class的__init__做簡單的實體化到記憶體。 Class上面比較細節的就不討論了，class decorator，@staticmethod或是jit之類的不是重點。 # MNIST example import torch import torch.nn as nn import torch.nn.functional as F from torchvision import datasets, transforms from complexLayers import ComplexBatchNorm2d, ComplexConv2d, ComplexLinear from complexFunctions import complex_relu, complex_max_pool2d batch_size = 64 trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) train_set = datasets.MNIST('../data', train=True, transform=trans, download=True) test_set = datasets.MNIST('../data', train=False, transform=trans, download=True) train_loader = torch.utils.data.DataLoader(train_set, batch_size= batch_size, shuffle=True) test_loader = torch.utils.data.DataLoader(test_set, batch_size= batch_size, shuffle=True) class ComplexNet(nn.Module): def __init__(self): super(ComplexNet, self).__init__() self.conv1 = ComplexConv2d(1, 20, 5, 1) self.bn = ComplexBatchNorm2d(20) self.conv2 = ComplexConv2d(20, 50, 5, 1) self.fc1 = ComplexLinear(4*4*50, 500) self.fc2 = ComplexLinear(500, 10) def forward(self,x): xr = x # imaginary part to zero xi = torch.zeros(xr.shape, dtype = xr.dtype, device = xr.device) xr,xi = self.conv1(xr,xi) xr,xi = complex_relu(xr,xi) xr,xi = complex_max_pool2d(xr,xi, 2, 2) xr,xi = self.bn(xr,xi) xr,xi = self.conv2(xr,xi) xr,xi = complex_relu(xr,xi) xr,xi = complex_max_pool2d(xr,xi, 2, 2) xr = xr.view(-1, 4*4*50) xi = xi.view(-1, 4*4*50) xr,xi = self.fc1(xr,xi) xr,xi = complex_relu(xr,xi) xr,xi = self.fc2(xr,xi) # take the absolute value as output x = torch.sqrt(torch.pow(xr,2)+torch.pow(xi,2)) return F.log_softmax(x, dim=1) # # Run training on 50 epochs # for epoch in range(50): # train(model, device, train_loader, optimizer, epoch) device = torch.device(\"cpu\" ) model = ComplexNet().to(device) optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) def train(model, device, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % 1000 == 0: print('Train Epoch: {:3} [{:6}/{:6} ({:3.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()) ) inputs,cla = next(iter(test_loader)) from torch.utils.tensorboard import SummaryWriter # Writer will output to ./runs/ directory by default writer = SummaryWriter() writer.add_graph(model, inputs) writer.close() from torchviz import make_dot, make_dot_from_trace # for param in model.named_parameters(): # print(param[0]) vis_graph = make_dot(model(inputs), params=dict(model.named_parameters())) vis_graph.view() # Digraph.gv.pdf 最後放一下產生的圖好了，不然感覺好像只有寫 Tensorboard\"\rTensorboard\r \r","date":"2020-09-14","objectID":"/2020-09-14/:5:0","tags":["python"],"title":"Pytorch視覺化","uri":"/2020-09-14/"},{"categories":null,"content":"It's a example","date":"2019-06-28","objectID":"/2020-06-28/","tags":["hugo"],"title":"ShortCode 整理","uri":"/2020-06-28/"},{"categories":null,"content":" 整理一下最近加入的shortcode，{{}}會錯誤所以都用{}代替，不然會變成這樣，今天換了Loveit發現主題內建了mermaid，所以就改檔案名，暫時擺著，順便加上Loveit有的shortcode，文檔有蠻多可以用的，就去不全部放了。 ","date":"2019-06-28","objectID":"/2020-06-28/:0:0","tags":["hugo"],"title":"ShortCode 整理","uri":"/2020-06-28/"},{"categories":null,"content":"Mindmap {\u003c mind \u003e} - 根目錄 - 一級目錄1 - 二級目錄1 - 二級目錄2 - 一級目錄2 {\u003c /mind \u003e} \r 根目录 一級目錄1 二級目錄1 二級目錄2 一級目錄2 \runordered-list-to-mind-map放到/static/mind/目錄下面，然後加入/layouts/shortcodes/mind.html的全局shortcode，關於這個shortcode我要說一下，在我這裡{{ .Inner }}不行用，要用{{ .Inner | markdownify }}，就看一下結果就知道了，就mindmap.js檔案寫的內容，如果可以改用#更好，不過\u003ch1\u003e\u003ch2\u003e\u003ch3\u003e這種tag看起來寫起來就很麻煩。 Inner\"\rInner\r Inner markdown\"\rInner markdown\r \u003c!-- from https://github.com/HunterXuan/unordered-list-to-mind-map --\u003e \u003cdiv id=\"{{ .Get 0 }}\" class=\"mindmap mindmap-lg\" style=\"width:100%;height:300px;border:3px #cccccc dashed;\"\u003e {{ .Inner | markdownify }} \u003c/div\u003e \u003cscript src=\"https://code.jquery.com/jquery-3.3.1.min.js\"\u003e\u003c/script\u003e \u003cscript src=\"/mind/jquery-3.3.1.min.js\"\u003e\u003c/script\u003e \u003clink href=\"/mind/mindmap.css\" rel=\"stylesheet\"\u003e \u003cscript src=\"/mind/kity.min.js\"\u003e\u003c/script\u003e \u003cscript src=\"/mind/kityminder.core.min.js\"\u003e\u003c/script\u003e \u003c!-- \u003cscript src=\"/mind/mindmap.min.js\"\u003e\u003c/script\u003e --\u003e \u003c!-- minder 重複加載會顯示我們不想要的結果選一個不加載或是直接刪除檔案 --\u003e \u003c!-- \u003cscript src=\"/mind/mindmap.js\"\u003e\u003c/script\u003e --\u003e 再來就是mindmap.js重複加載的問題，這個檔案重複加載，也就是畫了兩個圖會出現問題，直接變成四個圖，所以要處理一下，出現了圖重複的狀況。 我是直接對kityminder、kitty這兩個物件名稱做判斷（在瀏覽器找到就直接用了），原因就是每次會用到它的時候大概也是用在這裡，不過似乎不行，可能跟hugo的內部執行順序有關，所以最後還是加到theme裡面，我自己是加載在theme/layout/posts/single.html的\u003c/article\u003e上面一點的地方。 \u003cscript\u003e if (typeof jQuery != \"undefined\" || typeof kityminder != \"undefined\") { // do something 不能用document.write var s=document.createElement('script'); s.src='/mind/mindmap.min.js'; document.body.appendChild(s); } \u003c/script\u003e ","date":"2019-06-28","objectID":"/2020-06-28/:1:0","tags":["hugo"],"title":"ShortCode 整理","uri":"/2020-06-28/"},{"categories":null,"content":"Typeit 打字 TypeIt 的 打字動畫 的 段落…| {\u003c typeit \u003e} 打字 [TypeIt](https://typeitjs.com/) 的 **打字動畫** 的 *段落*... {\u003c /typeit \u003e} {\u003c typeit code=java \u003e} public class HelloWorld { public static void main(String []args) { System.out.println(\"Hello World\"); } } {\u003c /typeit \u003e} 下面用 {\u003c typeit code=C# \u003e}，{\u003c typeit code=python \u003e} 測試 ","date":"2019-06-28","objectID":"/2020-06-28/:2:0","tags":["hugo"],"title":"ShortCode 整理","uri":"/2020-06-28/"},{"categories":null,"content":"Plotly .embed-container { position: relative; padding-bottom: 56.25%; height: 100%; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }\r\r\r {\u003c plotly ns3_ofdm_yans_wifi_model_subplot \u003e} ","date":"2019-06-28","objectID":"/2020-06-28/:3:0","tags":["hugo"],"title":"ShortCode 整理","uri":"/2020-06-28/"},{"categories":null,"content":"Music use Roland JV-1080 vst, kontakt somthing wrong add other kontakt with Roland JV-1080 vst {\u003c music url=\"/audio/abelton_remix.wav\" name=\"Remix from abelton Live\" artist=NaN cover=\"/cover/avator.jpg\" \u003e} ","date":"2019-06-28","objectID":"/2020-06-28/:4:0","tags":["hugo"],"title":"ShortCode 整理","uri":"/2020-06-28/"},{"categories":null,"content":"Mapbox 再找個時間看一下語言設定，座標與Google Map複製的座標貼上順序相反。 {\u003c mapbox 121.56061923804162 25.04417192846157 10 false \"mapbox://styles/mapbox/streets-zh-v1\" \u003e} Zh style {\u003c mapbox -122.252 37.453 10 false \"mapbox://styles/mapbox/streets-zh-v1\" \u003e} Eng style {\u003c mapbox -122.252 37.453 10 false \"mapbox://styles/mapbox/streets-v11\" \u003e} ","date":"2019-06-28","objectID":"/2020-06-28/:5:0","tags":["hugo"],"title":"ShortCode 整理","uri":"/2020-06-28/"},{"categories":null,"content":"MS view \u003cbr\u003e \u003ciframe src='https://view.officeapps.live.com/op/embed.aspx?src=your_URL' width='98%' height='500px' frameborder='0'\u003e \u003c/iframe\u003e \u003cbr\u003e Office file ('.ppt’ ‘.pptx’ ‘.doc’, ‘.docx’, ‘.xls’, ‘.xlsx’) ","date":"2019-06-28","objectID":"/2020-06-28/:6:0","tags":["hugo"],"title":"ShortCode 整理","uri":"/2020-06-28/"},{"categories":null,"content":"Google View \u003cbr\u003e \u003ciframe src='https://docs.google.com/viewer?url=you_URL\u0026embedded=true' width='98%' height='700px' frameborder='0'\u003e \u003c/iframe\u003e \u003cbr\u003e Image files (.JPEG, .PNG, .GIF, .TIFF, .BMP) Video files (WebM, .MPEG4, .3GPP, .MOV, .AVI, .MPEGPS, .WMV, .FLV) Text files (.TXT) Markup/Code (.CSS, .HTML, .PHP, .C, .CPP, .H, .HPP, .JS) Microsoft Word (.DOC and .DOCX) Microsoft Excel (.XLS and .XLSX) Microsoft PowerPoint (.PPT and .PPTX) Adobe Portable Document Format (.PDF) Apple Pages (.PAGES) Adobe Illustrator (.AI) Adobe Photoshop (.PSD) Tagged Image File Format (.TIFF) Autodesk AutoCad (.DXF) Scalable Vector Graphics (.SVG) PostScript (.EPS, .PS) TrueType (.TTF) XML Paper Specification (.XPS) Archive file types (.ZIP and .RAR) ","date":"2019-06-28","objectID":"/2020-06-28/:7:0","tags":["hugo"],"title":"ShortCode 整理","uri":"/2020-06-28/"},{"categories":null,"content":"Hello Ha~ ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"關於 shihchun","uri":"/about/"}]